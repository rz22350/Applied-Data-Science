{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install Dependencies (Colab)\n",
        "!pip install torch torchvision --quiet\n",
        "\n",
        "# Step 2: Import Libraries\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "import random\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Step 3: Mount Google Drive & Extract Dataset\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dataset_zip_path = \"/content/drive/My Drive/snapvision_dataset.zip\"\n",
        "extract_folder = \"/content/snapvision_dataset\"\n",
        "\n",
        "os.makedirs(extract_folder, exist_ok=True)\n",
        "with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_folder)\n",
        "\n",
        "print(f\"Dataset extracted to: {extract_folder}\")\n",
        "\n",
        "dataset_path = \"/content/snapvision_dataset/uob_image_set\"\n",
        "\n",
        "# Step 4: Define Dataset Class\n",
        "class FashionDataset(Dataset):\n",
        "    def __init__(self, items, item_ids, transform=None, mode=\"train\"):\n",
        "        self.items = items\n",
        "        self.item_ids = item_ids\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "\n",
        "        if mode == \"train\":\n",
        "            self.flat_images = [(imgs, item_id) for imgs, item_id in zip(items, item_ids)]\n",
        "        else:\n",
        "            self.flat_images = [(img, item_id) for imgs, item_id in zip(items, item_ids) for img in imgs]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.flat_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode == \"train\":\n",
        "            images, item_id = self.flat_images[idx]  # Use actual folder name as ID\n",
        "\n",
        "            if len(images) == 0:\n",
        "              raise ValueError(f\"No images found for item_id {item_id}\")\n",
        "\n",
        "            anchor_img = Image.open(random.choice(images)).convert(\"RGB\")\n",
        "            positive_img = Image.open(random.choice(images)).convert(\"RGB\")\n",
        "\n",
        "            negative_idx = idx\n",
        "            while negative_idx == idx:\n",
        "              negative_idx = random.randint(0, len(self.flat_images) - 1)\n",
        "\n",
        "            negative_images, negative_id = self.flat_images[negative_idx]\n",
        "            negative_img = Image.open(random.choice(negative_images)).convert(\"RGB\")\n",
        "\n",
        "            if self.transform:\n",
        "                anchor_img = self.transform(anchor_img)\n",
        "                positive_img = self.transform(positive_img)\n",
        "                negative_img = self.transform(negative_img)\n",
        "\n",
        "            return anchor_img, positive_img, negative_img, item_id\n",
        "\n",
        "        else:  # Test mode\n",
        "            img_path, item_id = self.flat_images[idx]\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "\n",
        "            return img, item_id\n",
        "\n",
        "# Step 5: Data Preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),  # Add flips\n",
        "    transforms.RandomRotation(10),  # Rotate slightly\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Vary colors\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Step 6: Load Dataset & Split into Train/Validation/Test\n",
        "all_items = []\n",
        "item_ids = []\n",
        "\n",
        "for clothing_item in sorted(os.listdir(dataset_path)):\n",
        "    item_path = os.path.join(dataset_path, clothing_item)\n",
        "    if os.path.isdir(item_path):\n",
        "        images = [os.path.join(item_path, img) for img in sorted(os.listdir(item_path)) if img.endswith(\".jpg\")]\n",
        "        if len(images) >= 2:\n",
        "            all_items.append(images)\n",
        "            item_ids.append(clothing_item)\n",
        "\n",
        "# 80-10-10 Split (Clothing Item Level)\n",
        "train_size = int(0.8 * len(all_items))\n",
        "val_size = int(0.1 * len(all_items))\n",
        "test_size = len(all_items) - train_size - val_size\n",
        "\n",
        "# Split both `all_items` and `item_ids` consistently (instead of using random_split)\n",
        "train_items, val_items, test_items = all_items[:train_size], all_items[train_size:train_size+val_size], all_items[train_size+val_size:]\n",
        "train_ids, val_ids, test_ids = item_ids[:train_size], item_ids[train_size:train_size+val_size], item_ids[train_size+val_size:]\n",
        "\n",
        "train_dataset = FashionDataset(train_items, train_ids, transform=transform, mode=\"train\")\n",
        "val_dataset = FashionDataset(val_items, val_ids, transform=transform, mode=\"test\")\n",
        "test_dataset = FashionDataset(test_items, test_ids, transform=transform, mode=\"test\")\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# Step 7: Define the Embedding Model\n",
        "class EmbeddingNet(nn.Module):\n",
        "    def __init__(self, embedding_dim=256):  # Increased embedding dim\n",
        "        super(EmbeddingNet, self).__init__()\n",
        "        self.backbone = models.resnet50(pretrained=True)\n",
        "\n",
        "        # Fine-tune the entire ResNet50 model\n",
        "        # for param in self.backbone.parameters():\n",
        "        #     param.requires_grad = True  # Unfreeze all layers for fine-tuning\n",
        "        for param in list(self.backbone.parameters())[:-5]:\n",
        "            param.requires_grad = False  # Freeze first layers, train only last 5\n",
        "\n",
        "        # Replace classification layer with custom embedding layer\n",
        "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "# Step 8: Define Triplet Loss\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.loss_fn = nn.TripletMarginLoss(margin=self.margin, p=2)\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        return self.loss_fn(anchor, positive, negative)\n",
        "\n",
        "# Step 9: Initialize Model, Loss, and Optimizer\n",
        "model = EmbeddingNet().to(device)\n",
        "criterion = TripletLoss(margin=1.0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "\n",
        "# Step 10: Training Loop with Validation\n",
        "def get_hard_negative(anchor_emb, batch_emb, batch_labels, anchor_label):\n",
        "    \"\"\" Find the hardest negative example in the batch \"\"\"\n",
        "    anchor_emb_cpu = anchor_emb.reshape(1, -1).detach().cpu().numpy()\n",
        "    batch_emb_cpu = batch_emb.detach().cpu().numpy()\n",
        "\n",
        "    similarities = cosine_similarity(anchor_emb_cpu, batch_emb_cpu)[0]\n",
        "\n",
        "    negatives = [(similarities[i], batch_emb_cpu[i]) for i, lbl in enumerate(batch_labels) if lbl != anchor_label]\n",
        "\n",
        "    if negatives:\n",
        "        hard_negative = min(negatives, key=lambda x: x[0])[1]\n",
        "        return torch.tensor(hard_negative, dtype=torch.float32).to(device)\n",
        "    return None\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for anchor, positive, negative, batch_labels in train_dataloader:\n",
        "            anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "            batch_labels = [str(lbl) for lbl in batch_labels]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            anchor_emb = model(anchor)\n",
        "            positive_emb = model(positive)\n",
        "            batch_emb = model(negative)  # Process negatives as batch\n",
        "\n",
        "            # Replace random negative with hardest negative\n",
        "            hard_negatives = []\n",
        "            for i in range(anchor.shape[0]):\n",
        "                hard_negative = get_hard_negative(anchor_emb[i], batch_emb, batch_labels, batch_labels[i])\n",
        "                if hard_negative is not None:\n",
        "                    hard_negatives.append(hard_negative)\n",
        "\n",
        "            if hard_negatives:\n",
        "                hard_negatives = torch.stack(hard_negatives)\n",
        "                loss = criterion(anchor_emb, positive_emb, hard_negatives)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_dataloader):.4f}\")\n",
        "\n",
        "# Step 11: Train the Model\n",
        "train(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs=30)\n",
        "\n",
        "# Step 12: Save the Model\n",
        "torch.save(model.state_dict(), \"fashion_embedding_model.pth\")\n",
        "\n",
        "# Step 13: Model Evaluation (Top-1 Accuracy)\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_embeddings(model, dataloader):\n",
        "    model.eval()\n",
        "    embeddings = None\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, item_ids in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
        "            images = images.to(device)\n",
        "            emb = model(images).cpu().numpy()\n",
        "\n",
        "            if embeddings is None:\n",
        "                embeddings = emb\n",
        "            else:\n",
        "                embeddings = np.concatenate((embeddings, emb), axis=0)\n",
        "\n",
        "            labels.extend(item_ids)\n",
        "\n",
        "    return embeddings, np.array(labels)\n",
        "\n",
        "def evaluate_model(model, test_dataloader):\n",
        "    print(\"\\nEvaluating Model...\")\n",
        "\n",
        "    # Extract embeddings and item IDs\n",
        "    test_embeddings, test_labels = extract_embeddings(model, test_dataloader)\n",
        "\n",
        "    # Compute cosine similarity matrix\n",
        "    similarity_matrix = cosine_similarity(test_embeddings)\n",
        "\n",
        "    # Evaluate Top-1 retrieval accuracy\n",
        "    num_correct = 0\n",
        "    num_samples = similarity_matrix.shape[0]\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # Find most similar item (excluding itself)\n",
        "        sorted_indices = np.argsort(similarity_matrix[i])[::-1]  # Descending order\n",
        "        top_match = sorted_indices[1]  # First match after self (index 0)\n",
        "\n",
        "        # Check if retrieved image belongs to the same item\n",
        "        if test_labels[i] == test_labels[top_match]:  # Compare item IDs\n",
        "            num_correct += 1\n",
        "\n",
        "    accuracy = num_correct / num_samples\n",
        "    print(f\"✅ Top-1 Retrieval Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Run Evaluation\n",
        "evaluate_model(model, test_dataloader)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj3yVPWCT7ch",
        "outputId": "38965cf1-83fc-4af7-8b2e-ba4b1290dc67"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset extracted to: /content/snapvision_dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Loss: 0.0972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/30], Loss: 0.0693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/30], Loss: 0.0431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/30], Loss: 0.0463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/30], Loss: 0.0336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/30], Loss: 0.0396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/30], Loss: 0.0323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/30], Loss: 0.0377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/30], Loss: 0.0368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/30], Loss: 0.0402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/30], Loss: 0.0228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/30], Loss: 0.0430\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/30], Loss: 0.0347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/30], Loss: 0.0379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/30], Loss: 0.0377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/30], Loss: 0.0334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/30], Loss: 0.0328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/30], Loss: 0.0248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/30], Loss: 0.0435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/30], Loss: 0.0338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/30], Loss: 0.0259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/30], Loss: 0.0219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/30], Loss: 0.0186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/30], Loss: 0.0229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/30], Loss: 0.0254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26/30], Loss: 0.0237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [27/30], Loss: 0.0348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [28/30], Loss: 0.0302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [29/30], Loss: 0.0289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/30], Loss: 0.0270\n",
            "\n",
            "Evaluating Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rExtracting embeddings:   0%|          | 0/21 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Extracting embeddings: 100%|██████████| 21/21 [00:12<00:00,  1.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Top-1 Retrieval Accuracy: 44.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tl8GFeFsdrjK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}